import 'dart:async';
import 'dart:io';
import 'dart:isolate';

import 'package:camera/camera.dart';
import 'package:flutter/services.dart';
import 'package:google_mlkit_face_detection/google_mlkit_face_detection.dart';
import 'package:image/image.dart' as imglib;
import 'app_config.dart';

@pragma('vm:entry-point')
Future<void> processImage(List<Object> args) async {
  try {
    SendPort sendPort = args[0] as SendPort;
    RootIsolateToken rootIsolateToken = args[1] as RootIsolateToken;
    BackgroundIsolateBinaryMessenger.ensureInitialized(rootIsolateToken);

    // T·∫°o ReceivePort ƒë·ªÉ nh·∫≠n d·ªØ li·ªáu t·ª´ main isolate
    ReceivePort imagePort = ReceivePort();
    // G·ª≠i SendPort c·ªßa isolate n√†y v·ªÅ main isolate
    sendPort.send(imagePort.sendPort);

    final FaceDetector faceDetector = FaceDetector(
      options: FaceDetectorOptions(
        enableContours: true,
        //B·∫≠t ph√°t hi·ªán ƒë∆∞·ªùng vi·ªÅn khu√¥n m·∫∑t.
        enableClassification: true,
        //B·∫≠t ph√¢n lo·∫°i (v√≠ d·ª•: x√°c ƒë·ªãnh c·∫£m x√∫c ho·∫∑c tr·∫°ng th√°i m·∫Øt).
        enableTracking: true,
        //B·∫≠t theo d√µi khu√¥n m·∫∑t qua c√°c frame.
        performanceMode:
            FaceDetectorMode
                .accurate, //Ch·ªçn ch·∫ø ƒë·ªô ch√≠nh x√°c cao (thay v√¨ nhanh).
      ),
    );

    //X·ª≠ l√Ω d·ªØ li·ªáu h√¨nh ·∫£nh
    await for (var message in imagePort) {
      app_config.printLog(
        'i',
        '[Isolate] message received: '
            '[33m$message[0m',
      );
      if (message is List) {
        app_config.printLog(
          'i',
          '[Isolate] message length: [36m${message.length}[0m',
        );
        if (message.isNotEmpty && message.length == 3) {
          if (message[0] is CameraImage &&
              message[1] is int &&
              message[2] is SendPort) {
            final CameraImage image = message[0];
            final int sensorOrientation = message[1];
            final SendPort sendMsg = message[2];

            // Ki·ªÉm tra s·ªë l∆∞·ª£ng planes
            if (image.planes.length < 3) {
              app_config.printLog(
                'e',
                '[Error Camera] CameraImage does not have 3 planes!',
              );
              continue;
            }
            // Ki·ªÉm tra format
            if (image.format.group != ImageFormatGroup.yuv420) {
              app_config.printLog(
                'e',
                '[Error Camera] CameraImage format is not yuv420!',
              );
              continue;
            }

            // Chuy·ªÉn sensorOrientation th√†nh InputImageRotation
            InputImageRotation rotation;
            switch (sensorOrientation) {
              case 0:
                rotation = InputImageRotation.rotation0deg;
                break;
              case 90:
                rotation = InputImageRotation.rotation90deg;
                break;
              case 180:
                rotation = InputImageRotation.rotation180deg;
                break;
              case 270:
                rotation = InputImageRotation.rotation270deg;
                break;
              default:
                rotation = InputImageRotation.rotation0deg;
            }

            // Chuy·ªÉn CameraImage (3 planes) sang NV21 ƒë√∫ng chu·∫©n
            Uint8List nv21Bytes = convertYUV420ToNV21Safe(image);
            InputImage inputImage = InputImage.fromBytes(
              bytes: nv21Bytes,
              metadata: InputImageMetadata(
                size: Size(image.width.toDouble(), image.height.toDouble()),
                format: InputImageFormat.nv21,
                bytesPerRow: image.planes[0].bytesPerRow,
                rotation: rotation,
              ),
            );

            List<Face> faces = await faceDetector.processImage(inputImage);
            app_config.printLog(
              'i',
              '[Debug camera] faces : [32m${faces.length}[0m',
            );
            imglib.Image img = decodeNV21(inputImage);
            sendMsg.send([
              faces,
              img,
              inputImage.metadata!.size,
              inputImage.metadata!.rotation,
            ]);
          }
        } else {
          app_config.printLog('e', '[Isolate] message format invalid!');
        }
      } else {
        app_config.printLog('e', '[Isolate] message is not a List!');
      }
    }
  } catch (e) {
    app_config.printLog('e', '[Error Camera] $e');
  }
}

Uint8List convertYUV420ToNV21Safe(CameraImage image) {
  final int width = image.width;
  final int height = image.height;

  final int ySize = width * height;
  final int uvSize = width * height ~/ 2;
  final Uint8List nv21 = Uint8List(ySize + uvSize);

  final Uint8List yPlane = image.planes[0].bytes;
  final Uint8List uPlane = image.planes[1].bytes;
  final Uint8List vPlane = image.planes[2].bytes;

  final int yRowStride = image.planes[0].bytesPerRow;

  // Copy Y plane
  int destIndex = 0;
  for (int row = 0; row < height; row++) {
    final int srcIndex = row * yRowStride;
    nv21.setRange(destIndex, destIndex + width, yPlane, srcIndex);
    destIndex += width;
  }

  final int uvRowStride = image.planes[1].bytesPerRow;
  final int uvPixelStride = image.planes[1].bytesPerPixel!;

  // Interleave V and U planes to NV21 format (VU VU VU...)
  int uvStartIndex = ySize;
  for (int row = 0; row < height ~/ 2; row++) {
    for (int col = 0; col < width ~/ 2; col++) {
      final int uvIndex = row * uvRowStride + col * uvPixelStride;
      nv21[uvStartIndex++] = vPlane[uvIndex]; // V
      nv21[uvStartIndex++] = uPlane[uvIndex]; // U
    }
  }

  return nv21;
}

///Tr·∫£ v·ªÅ ƒë·ªëi t∆∞·ª£ng imglib.Image ch·ª©a h√¨nh ·∫£nh ƒë√£ ƒë∆∞·ª£c chuy·ªÉn ƒë·ªïi NV21 sang ƒë·ªãnh d·∫°ng RGB.
///B·∫°n n√™n d√πng decodeNV21(...) khi:
/// ƒê√£ c√≥ d·ªØ li·ªáu ·∫£nh d·∫°ng NV21 (Uint8List) ‚Äì th∆∞·ªùng sau khi d√πng convertYUV420(...).
/// Mu·ªën hi·ªÉn th·ªã ·∫£nh ho·∫∑c debug tr√™n m√†n h√¨nh.
/// Kh√¥ng c·∫ßn g·ªçi model n·ªØa m√† ch·ªâ c·∫ßn x·ª≠ l√Ω RGB
/// d√πng ƒë·ªÉ g·ª≠i ·∫£nh v·ªÅ server
imglib.Image decodeNV21(InputImage image) {
  final width = image.metadata!.size.width.toInt();
  final height = image.metadata!.size.height.toInt();

  Uint8List yuv420sp = image.bytes!;

  ///T·∫°o m·ªôt ƒë·ªëi t∆∞·ª£ng h√¨nh ·∫£nh m·ªõi v·ªõi k√≠ch th∆∞·ªõc t∆∞∆°ng ·ª©ng ƒë·ªÉ l∆∞u k·∫øt qu·∫£ RGB.
  final outImg = imglib.Image(width: width, height: height);

  ///K√™nh Y (luma) ch·ª©a th√¥ng tin ƒë·ªô s√°ng, chi·∫øm width * height byte.
  ///T√≠nh k√≠ch th∆∞·ªõc c·ªßa k√™nh Y.
  final int frameSize = width * height;

  for (int j = 0, yp = 0; j < height; j++) {
    int uvp = frameSize + (j >> 1) * width, u = 0, v = 0;
    for (int i = 0; i < width; i++, yp++) {
      int y = (0xff & yuv420sp[yp]) - 16;
      if (y < 0) y = 0;
      if ((i & 1) == 0) {
        v = (0xff & yuv420sp[uvp++]) - 128;
        u = (0xff & yuv420sp[uvp++]) - 128;
      }
      int y1192 = 1192 * y;
      int r = (y1192 + 1634 * v);
      int g = (y1192 - 833 * v - 400 * u);
      int b = (y1192 + 2066 * u);

      r = r.clamp(0, 262143);
      g = g.clamp(0, 262143);
      b = b.clamp(0, 262143);

      outImg.setPixelRgb(
        i,
        j,
        ((r << 6) & 0xff0000) >> 16,
        ((g >> 2) & 0xff00) >> 8,
        (b >> 10) & 0xff,
      );
    }
  }
  return outImg;
}

class APICamera {
  ///L∆∞u h∆∞·ªõng ban ƒë·∫ßu c·ªßa camera (v√≠ d·ª•: front ho·∫∑c back), ƒë∆∞·ª£c kh·ªüi t·∫°o sau khi g·ªçi constructor.
  late CameraLensDirection _initialDirection;

  ///Danh s√°ch c√°c camera kh·∫£ d·ª•ng tr√™n thi·∫øt b·ªã, ƒë∆∞·ª£c kh·ªüi t·∫°o sau.
  late List<CameraDescription> _cameras;

  ///Ch·ªâ s·ªë camera hi·ªán t·∫°i (m·∫∑c ƒë·ªãnh l√† 0).
  int _camera_index = 0;

  ///ƒê·ªëi t∆∞·ª£ng ƒëi·ªÅu khi·ªÉn camera (t√πy ch·ªçn, c√≥ th·ªÉ l√† null n·∫øu ch∆∞a kh·ªüi t·∫°o).
  CameraController? controller;

  late Isolate _isolate;
  late SendPort sendPort;
  final ReceivePort _receivePort = ReceivePort();

  ///C·ªù tr·∫°ng th√°i ƒë·ªÉ ki·ªÉm tra xem camera c√≥ ƒëang b·∫≠n kh√¥ng.
  bool _busy = false;

  ///C·ªù tr·∫°ng th√°i ƒë·ªÉ ki·ªÉm tra xem camera c√≥ ƒëang ch·∫°y kh√¥ng.
  bool _run = false;

  ///B·ªô ƒëi·ªÅu khi·ªÉn lu·ªìng ƒë·ªÉ ph√°t d·ªØ li·ªáu h√¨nh ·∫£nh JPEG t·ªõi c√°c subscriber.
  StreamController streamJpgController = StreamController.broadcast();

  ///B·ªô ƒëi·ªÅu khi·ªÉn lu·ªìng ƒë·ªÉ ph√°t d·ªØ li·ªáu ph√°t hi·ªán khu√¥n m·∫∑t t·ªõi c√°c subscriber.
  StreamController streamDectectFaceController = StreamController.broadcast();

  ///B·ªô ƒëi·ªÅu khi·ªÉn lu·ªìng ƒë·ªÉ g·ª≠i th√¥ng tin faces l√™n UI ƒë·ªÉ v·∫Ω bounding box.
  StreamController streamFacesForUI = StreamController.broadcast();

  APICamera(CameraLensDirection direction) {
    _initialDirection = direction;
  }

  Future<void> init(RootIsolateToken rootIsolateToken) async {
    try {
      ///T·∫°o m·ªôt c·ªïng nh·∫≠n ƒë·ªÉ nh·∫≠n th√¥ng ƒëi·ªáp t·ª´ isolate n·ªÅn khi n√≥ kh·ªüi ƒë·ªông.
      ReceivePort myReceivePort = ReceivePort();
      _isolate = await Isolate.spawn(processImage, [
        myReceivePort.sendPort,
        rootIsolateToken,
      ]);
      app_config.printLog('i', '[Debug] * * * * *');
      sendPort = await myReceivePort.first;
      app_config.printLog('i', '[Debug] * * * * * * * * * *');
      _receivePort.listen((message) {
        app_config.printLog('i', '[Debug camera] finish process image');
        if (message is List) {
          app_config.printLog('i', '[Debug camera] finish process image * ');
          if (message.isNotEmpty && message.length == 4) {
            app_config.printLog(
              'i',
              '[Debug camera] finish process image * * ',
            );
            if (message[0] is List<Face> &&
                message[1] is imglib.Image &&
                message[2] is Size &&
                message[3] is InputImageRotation) {
              ///L·∫•y danh s√°ch khu√¥n m·∫∑t ƒë∆∞·ª£c ph√°t hi·ªán.
              final List<Face> faces = message[0];

              ///L·∫•y h√¨nh ·∫£nh ƒë√£ x·ª≠ l√Ω.
              final imglib.Image img = message[1];
              final Size size = message[2];
              final InputImageRotation rotation = message[3];
              app_config.printLog(
                'i',
                '[Debug] size : [32m${faces.length}[0m',
              );
              streamDectectFaceController.sink.add([
                faces,
                img,
                size,
                rotation,
              ]);
              _busy = false;
            }
          }
        }
      });
      _busy = false;
      _run = false;
    } catch (e) {
      app_config.printLog('e', '[Error Camera] $e');
    }
  }

  Future<void> start() async {
    if (_run == false) {
      try {
        _cameras = await availableCameras();
        _camera_index = 0;
        if (_cameras.any(
          (element) =>
              element.lensDirection == _initialDirection &&
              element.sensorOrientation == 99,
        )) {
          _camera_index = _cameras.indexOf(
            _cameras.firstWhere(
              (element) =>
                  element.lensDirection == _initialDirection &&
                  element.sensorOrientation == 99,
            ),
          );
        } else {
          _camera_index = _cameras.indexOf(
            _cameras.firstWhere(
              (element) => element.lensDirection == _initialDirection,
            ),
          );
        }
        if (_cameras[_camera_index] != null) {
          if (controller != null) {
            await controller!.stopImageStream();
            controller = null;
          }
          controller = CameraController(
            _cameras[_camera_index],
            ResolutionPreset.medium,
            enableAudio: false,
            imageFormatGroup: ImageFormatGroup.yuv420,
          );
          if (controller != null) {
            try {
              await controller!.initialize();
              controller!.startImageStream((value) {
                CameraImage image = value;
                int sensorOrientation =
                    _cameras[_camera_index].sensorOrientation;
                if (_busy == false) {
                  _busy = true;
                  app_config.printLog(
                    'i',
                    '[Debug camera] start process image',
                  );
                  if (sendPort == null) {
                    app_config.printLog(
                      'i',
                      '[Debug camera] start process image * ',
                    );
                  }
                  sendPort.send([
                    image,
                    sensorOrientation,
                    _receivePort.sendPort,
                  ]);
                }
              });
              _run = true;
            } on CameraException catch (e) {
              switch (e.code) {
                case 'CameraAccessDenied':
                  app_config.printLog('e', 'You have denied camera access.');
                  break;
                case 'CameraAccessDeniedWithoutPrompt':
                  // iOS only
                  app_config.printLog(
                    'e',
                    'Please go to Settings app to enable camera access.',
                  );
                  break;
                case 'CameraAccessRestricted':
                  // iOS only
                  app_config.printLog('e', 'Camera access is restricted.');
                  break;
                case 'AudioAccessDenied':
                  app_config.printLog('e', 'You have denied audio access.');
                  break;
                case 'AudioAccessDeniedWithoutPrompt':
                  // iOS only
                  app_config.printLog(
                    'e',
                    'Please go to Settings app to enable audio access.',
                  );
                  break;
                case 'AudioAccessRestricted':
                  // iOS only
                  app_config.printLog('e', 'Audio access is restricted.');
                  break;
                default:
                  app_config.printLog('e', e.toString());
                  break;
              }
            }
          }
        }
      } catch (e) {
        app_config.printLog('e', '[Error Camera] $e');
      }
    }
  }

  Future<void> stop() async {
    if (_run == true) {
      try {
        if (controller != null) {
          await controller!.stopImageStream();
          await controller!.dispose();
          controller = null;
          _run = false;
        }
      } catch (e) {
        app_config.printLog('i', '[Debug] : $e');
      }
    }
  }

  bool state() {
    return _run;
  }

  /// YUV420 th√†nh ·∫£nh RGB
  /// Chuy·ªÉn CameraImage t·ª´ ƒë·ªãnh d·∫°ng YUV420 sang ·∫£nh imglib.Image (d√πng ƒë·ªÉ v·∫Ω, l∆∞u, nh·∫≠n di·ªán, v.v.).
  /// C·∫ßn hi·ªÉn th·ªã ·∫£nh ho·∫∑c debug pixel
  imglib.Image convertYUV420(CameraImage cameraImage) {
    /// L·∫•y chi·ªÅu r·ªông v√† chi·ªÅu cao c·ªßa h√¨nh ·∫£nh t·ª´ cameraImage.
    final imageWidth = cameraImage.width;
    final imageHeight = cameraImage.height;

    /// Plane Y (ƒë·ªô s√°ng), ch·ª©a d·ªØ li·ªáu cho m·ªói pixel.
    final yBuffer = cameraImage.planes[0].bytes;

    /// Plane U (chrominance, m√†u xanh), c√≥ k√≠ch th∆∞·ªõc nh·ªè h∆°n (th∆∞·ªùng l√† width/2 * height/2).
    final uBuffer = cameraImage.planes[1].bytes;

    /// Plane V (chrominance, m√†u ƒë·ªè), k√≠ch th∆∞·ªõc t∆∞∆°ng t·ª± U.
    final vBuffer = cameraImage.planes[2].bytes;

    /// S·ªë byte tr√™n m·ªói h√†ng c·ªßa plane Y. Th∆∞·ªùng b·∫±ng width, nh∆∞ng c√≥ th·ªÉ l·ªõn h∆°n n·∫øu c√≥ padding.
    final int yRowStride = cameraImage.planes[0].bytesPerRow;

    /// S·ªë byte cho m·ªói pixel trong plane Y (th∆∞·ªùng l√† 1, v√¨ Y l√† 8-bit grayscale).
    final int yPixelStride = cameraImage.planes[0].bytesPerPixel!;

    /// S·ªë byte tr√™n m·ªói h√†ng c·ªßa plane U/V.
    final int uvRowStride = cameraImage.planes[1].bytesPerRow;

    /// S·ªë byte cho m·ªói pixel trong plane U/V (th∆∞·ªùng l√† 1).
    final int uvPixelStride = cameraImage.planes[1].bytesPerPixel!;

    /// Kh·ªüi t·∫°o m·ªôt ƒë·ªëi t∆∞·ª£ng imglib.Image v·ªõi k√≠ch th∆∞·ªõc gi·ªëng CameraImage.
    /// imglib.Image l√† m·ªôt l·ªõp t·ª´ th∆∞ vi·ªán image (package image trong Dart), l∆∞u tr·ªØ h√¨nh ·∫£nh d∆∞·ªõi d·∫°ng pixel RGBA.
    /// Khi kh·ªüi t·∫°o, h√¨nh ·∫£nh ch∆∞a c√≥ d·ªØ li·ªáu pixel (t·∫•t c·∫£ pixel m·∫∑c ƒë·ªãnh l√† trong su·ªët ho·∫∑c ƒëen).
    final image = imglib.Image(width: imageWidth, height: imageHeight);

    /// L·∫∑p qua c√°c pixel ƒë·ªÉ t√≠nh gi√° tr·ªã RGB
    for (int h = 0; h < imageHeight; h++) {
      /// V√¨ U/V ƒë∆∞·ª£c l·∫•y m·∫´u (subsampled) v·ªõi ƒë·ªô ph√¢n gi·∫£i gi·∫£m m·ªôt n·ª≠a theo chi·ªÅu d·ªçc, m·ªói gi√° tr·ªã U/V √°p d·ª•ng cho 2 pixel theo chi·ªÅu cao.
      int uvh = (h / 2).floor();

      for (int w = 0; w < imageWidth; w++) {
        /// T∆∞∆°ng t·ª±, U/V gi·∫£m m·ªôt n·ª≠a theo chi·ªÅu ngang.
        int uvw = (w / 2).floor();

        /// X√°c ƒë·ªãnh v·ªã tr√≠ byte c·ªßa pixel (w, h) trong yBuffer.
        final yIndex = (h * yRowStride) + (w * yPixelStride);

        /// L·∫•y gi√° tr·ªã Y (ƒë·ªô s√°ng) t·∫°i v·ªã tr√≠ n√†y, n·∫±m trong kho·∫£ng [0, 255].
        final int y = yBuffer[yIndex];

        /// X√°c ƒë·ªãnh v·ªã tr√≠ byte c·ªßa gi√° tr·ªã U/V cho pixel (w, h).
        final int uvIndex = (uvh * uvRowStride) + (uvw * uvPixelStride);

        /// L·∫•y gi√° tr·ªã U, V, c≈©ng n·∫±m trong kho·∫£ng [0, 255].
        final int u = uBuffer[uvIndex];
        final int v = vBuffer[uvIndex];

        /// Chuy·ªÉn ƒë·ªïi YUV sang RGB
        int r = (y + v * 1436 / 1024 - 179).round();
        int g = (y - u * 46549 / 131072 + 44 - v * 93604 / 131072 + 91).round();
        int b = (y + u * 1814 / 1024 - 227).round();

        r = r.clamp(0, 255);
        g = g.clamp(0, 255);
        b = b.clamp(0, 255);
        image.setPixelRgba(w, h, r, g, b, 255);
      }
    }
    return image;
  }

  imglib.Image convertBGRA8888(CameraImage image) {
    return imglib.Image.fromBytes(
      width: image.width,
      height: image.height,
      bytes: image.planes[0].bytes.buffer,
    );
  }

  Uint8List convertJPG(imglib.Image image) {
    return Uint8List.fromList(imglib.encodeJpg(image, quality: 90));
  }
}
